{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea.\n",
    "We will use as much class inheritance as possible so we are making few changes.\n",
    "\n",
    "We will be copying from the source code we downloaded for version 1.8.0 of Stable Baselines3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import MultiDiscrete\n",
    "from gym.spaces import Discrete\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# https://www.youtube.com/watch?v=R5S2FmtFnt8&ab_channel=DibyaChakravorty\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Need to define observation and action space.\n",
    "        action_lows = np.array([1, 0, 0, 0.05])\n",
    "        action_highs = np.array([4, 2, 4, 10])\n",
    "        self.action_space = Box(low=action_lows, high=action_highs)\n",
    "\n",
    "        # Observation space will have just t and r. Both are floats. t is in [0, 100] and r is in [-100, 100].\n",
    "        obs_lows = np.array([0, -100])\n",
    "        obs_highs = np.array([100, 100])\n",
    "        self.observation_space = Box(low=obs_lows, high=obs_highs)\n",
    "\n",
    "        # Also declare a random number generator.\n",
    "        self.rng = default_rng()\n",
    "\n",
    "        # This will always store the current observation.\n",
    "        self.current_obs = None\n",
    "    \n",
    "    def reset(self):\n",
    "        # The reset() method will initialise an episode (fix the problem parameters by sampling from the parameter space).\n",
    "        # Each episode, we will have different starting observation space.\n",
    "        normal = min(100, max(0, self.rng.normal(loc=48.57142857, scale=15.89249598)))\n",
    "        uniform = self.rng.uniform(-100, 100)\n",
    "\n",
    "        # Return the first (initial) observation.\n",
    "        self.current_obs = np.array([normal, uniform], dtype=\"float32\")\n",
    "\n",
    "        return self.current_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # The input \"action\" will be a numpy array like np.array([2, 3, 2, 4]).\n",
    "        c_r = action[0]\n",
    "        m_r = action[1]\n",
    "        s_r = action[2]\n",
    "        p = action[3]\n",
    "\n",
    "        normal, uniform = self.current_obs\n",
    "\n",
    "        # Need to calculate the next observation (even though wont need it).\n",
    "        # Dont have randomness, have a deterministic value for r_gain and prof as just using regression. Sals is bounded by 0.\n",
    "        sals = max(0, 44.06153 + -4.42618 * p + 0.167923 * uniform + -0.31689 * normal)\n",
    "\n",
    "        r_per_sal = -0.13906 + 0.067546 * c_r + 0.266495 * m_r + 0.097398 * s_r + -0.20013 * p\n",
    "        r_at_end = min(100, max(-100, uniform + sals * r_per_sal))\n",
    "        r_gain = r_at_end - uniform\n",
    "\n",
    "        prof = sals * p - sals * (0.3 * c_r + 0.09 * m_r + 0.16 * s_r + 0.12)\n",
    "\n",
    "        self.current_obs = np.array([normal, r_at_end], dtype=\"float32\")\n",
    "\n",
    "        # Need to calculate reward. Will just have 1 reward for now, prof.\n",
    "        reward = np.array([prof, r_gain])\n",
    "\n",
    "        # Need to compute done (always True).\n",
    "        done = True\n",
    "\n",
    "        return self.current_obs, reward, done, {}\n",
    "\n",
    "\n",
    "class CustomEnvSing(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Need to define observation and action space.\n",
    "        action_lows = np.array([1, 0, 0, 0.05])\n",
    "        action_highs = np.array([4, 2, 4, 10])\n",
    "        self.action_space = Box(low=action_lows, high=action_highs)\n",
    "\n",
    "        # Observation space will have just t and r. Both are floats. t is in [0, 100] and r is in [-100, 100].\n",
    "        obs_lows = np.array([0, -100])\n",
    "        obs_highs = np.array([100, 100])\n",
    "        self.observation_space = Box(low=obs_lows, high=obs_highs)\n",
    "\n",
    "        # Also declare a random number generator.\n",
    "        self.rng = default_rng()\n",
    "\n",
    "        # This will always store the current observation.\n",
    "        self.current_obs = None\n",
    "    \n",
    "    def reset(self):\n",
    "        # The reset() method will initialise an episode (fix the problem parameters by sampling from the parameter space).\n",
    "        # Each episode, we will have different starting observation space.\n",
    "        normal = min(100, max(0, self.rng.normal(loc=48.57142857, scale=15.89249598)))\n",
    "        uniform = self.rng.uniform(-100, 100)\n",
    "\n",
    "        # Return the first (initial) observation.\n",
    "        self.current_obs = np.array([normal, uniform], dtype=\"float32\")\n",
    "\n",
    "        return self.current_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # The input \"action\" will be a numpy array like np.array([2, 3, 2, 4]).\n",
    "        c_r = action[0]\n",
    "        m_r = action[1]\n",
    "        s_r = action[2]\n",
    "        p = action[3]\n",
    "\n",
    "        normal, uniform = self.current_obs\n",
    "\n",
    "        # Need to calculate the next observation (even though wont need it).\n",
    "        # Dont have randomness, have a deterministic value for r_gain and prof as just using regression. Sals is bounded by 0.\n",
    "        sals = max(0, 44.06153 + -4.42618 * p + 0.167923 * uniform + -0.31689 * normal)\n",
    "\n",
    "        r_per_sal = -0.13906 + 0.067546 * c_r + 0.266495 * m_r + 0.097398 * s_r + -0.20013 * p\n",
    "        r_at_end = min(100, max(-100, uniform + sals * r_per_sal))\n",
    "        r_gain = r_at_end - uniform\n",
    "\n",
    "        prof = sals * p - sals * (0.3 * c_r + 0.09 * m_r + 0.16 * s_r + 0.12)\n",
    "\n",
    "        self.current_obs = np.array([normal, r_at_end], dtype=\"float32\")\n",
    "\n",
    "        # Need to calculate reward. Will just have 1 reward for now, prof.\n",
    "        reward = prof\n",
    "\n",
    "        # Need to compute done (always True).\n",
    "        done = True\n",
    "\n",
    "        return self.current_obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PPO class.\n",
    "This class will be exactly the same (via inheritance) but we need to change the MlpExtractor class to one of my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from typing import Dict, List, Tuple, Type, Union\n",
    "\n",
    "class CustomMlpExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        device = get_device(device)\n",
    "        \n",
    "        print(\"In CustomMlpExtractor\")\n",
    "\n",
    "        # Policy net.\n",
    "        policy_ray_hidden_dim=100\n",
    "        self.policy_ray_mlp = nn.Sequential(\n",
    "            nn.Linear(2, policy_ray_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(policy_ray_hidden_dim, policy_ray_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(policy_ray_hidden_dim, policy_ray_hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.policy_targetnet_in_dim = 2\n",
    "        self.policy_targetnet_dims = [64, 64]\n",
    "        \n",
    "        prvs_dim = self.policy_targetnet_in_dim\n",
    "        for i, dim in enumerate(self.policy_targetnet_dims):\n",
    "            setattr(self, f\"policy_fc_{i}_weights\", nn.Linear(policy_ray_hidden_dim, prvs_dim * dim))\n",
    "            setattr(self, f\"policy_fc_{i}_bias\", nn.Linear(policy_ray_hidden_dim, dim))\n",
    "            prvs_dim = dim\n",
    "        \n",
    "        # Value net.\n",
    "        value_ray_hidden_dim=100\n",
    "        self.value_ray_mlp = nn.Sequential(\n",
    "            nn.Linear(2, value_ray_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(value_ray_hidden_dim, value_ray_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(value_ray_hidden_dim, value_ray_hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.value_targetnet_in_dim = 2\n",
    "        self.value_targetnet_dims = [64, 64]\n",
    "        \n",
    "        prvs_dim = self.value_targetnet_in_dim\n",
    "        for i, dim in enumerate(self.value_targetnet_dims):\n",
    "            setattr(self, f\"value_fc_{i}_weights\", nn.Linear(value_ray_hidden_dim, prvs_dim * dim))\n",
    "            setattr(self, f\"value_fc_{i}_bias\", nn.Linear(value_ray_hidden_dim, dim))\n",
    "            prvs_dim = dim\n",
    "            \n",
    "\n",
    "        # Save dim, used to create the distributions\n",
    "        self.latent_dim_pi = 64\n",
    "        self.latent_dim_vf = 64\n",
    "    \n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        out_dict = dict()\n",
    "        hypnet_output = self.policy_ray_mlp(th.Tensor([0.5, 0.5]))\n",
    "\n",
    "        prvs_dim = self.policy_targetnet_in_dim\n",
    "        for i, dim in enumerate(self.policy_targetnet_dims):\n",
    "            out_dict[f\"policy_fc_{i}_weights\"] = self.__getattr__(f\"policy_fc_{i}_weights\")(hypnet_output).reshape(dim, prvs_dim)\n",
    "            out_dict[f\"policy_fc_{i}_bias\"] = self.__getattr__(f\"policy_fc_{i}_bias\")(hypnet_output).flatten()\n",
    "            prvs_dim = dim\n",
    "\n",
    "        tnet = MyPolicyTargetFCNet()\n",
    "        tnet_output = tnet(features, out_dict)\n",
    "        return tnet_output\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        out_dict = dict()\n",
    "        hypnet_output = self.value_ray_mlp(th.Tensor([0.5, 0.5]))\n",
    "\n",
    "        prvs_dim = self.value_targetnet_in_dim\n",
    "        for i, dim in enumerate(self.value_targetnet_dims):\n",
    "            out_dict[f\"value_fc_{i}_weights\"] = self.__getattr__(f\"value_fc_{i}_weights\")(hypnet_output).reshape(dim, prvs_dim)\n",
    "            out_dict[f\"value_fc_{i}_bias\"] = self.__getattr__(f\"value_fc_{i}_bias\")(hypnet_output).flatten()\n",
    "            prvs_dim = dim\n",
    "\n",
    "        tnet = MyValueTargetFCNet()\n",
    "        tnet_output = tnet(features, out_dict)\n",
    "        return tnet_output\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class MyPolicyTargetFCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, weights):\n",
    "        for i in range(int(len(weights) / 2)):\n",
    "            x = F.linear(x, weights[f\"policy_fc_{i}_weights\"], weights[f\"policy_fc_{i}_bias\"])\n",
    "            if i < int(len(weights) / 2) - 1:\n",
    "                x = F.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyValueTargetFCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, weights):\n",
    "        for i in range(int(len(weights) / 2)):\n",
    "            x = F.linear(x, weights[f\"value_fc_{i}_weights\"], weights[f\"value_fc_{i}_bias\"])\n",
    "            if i < int(len(weights) / 2) - 1:\n",
    "                x = F.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import (BaseFeaturesExtractor, FlattenExtractor)\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "from stable_baselines3.common.utils import get_device\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from stable_baselines3.common.distributions import (\n",
    "    BernoulliDistribution,\n",
    "    CategoricalDistribution,\n",
    "    DiagGaussianDistribution,\n",
    "    MultiCategoricalDistribution,\n",
    "    StateDependentNoiseDistribution,\n",
    ")\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        ortho_init: bool = True,\n",
    "        use_sde: bool = False,\n",
    "        log_std_init: float = 0.0,\n",
    "        full_std: bool = True,\n",
    "        use_expln: bool = False,\n",
    "        squash_output: bool = False,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        share_features_extractor: bool = True,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        env_num_rewards = None\n",
    "    ):\n",
    "        if optimizer_kwargs is None:\n",
    "            optimizer_kwargs = {}\n",
    "            # Small values to avoid NaN in Adam optimizer\n",
    "            if optimizer_class == th.optim.Adam:\n",
    "                optimizer_kwargs[\"eps\"] = 1e-5\n",
    "        \n",
    "        self.env_num_rewards = env_num_rewards\n",
    "        \n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            ortho_init,\n",
    "            use_sde,\n",
    "            log_std_init,\n",
    "            full_std,\n",
    "            use_expln,\n",
    "            squash_output,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            share_features_extractor,\n",
    "            normalize_images,\n",
    "            optimizer_class,\n",
    "            optimizer_kwargs\n",
    "        )\n",
    "    \n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        \"\"\"\n",
    "        Create the policy and value networks.\n",
    "        Part of the layers can be shared.\n",
    "        \"\"\"\n",
    "        self.mlp_extractor = CustomMlpExtractor(\n",
    "            self.features_dim,\n",
    "            net_arch=self.net_arch,\n",
    "            activation_fn=self.activation_fn,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        \"\"\"\n",
    "        Create the networks and the optimizer.\n",
    "\n",
    "        :param lr_schedule: Learning rate schedule\n",
    "            lr_schedule(1) is the initial learning rate\n",
    "        \"\"\"\n",
    "        self._build_mlp_extractor()\n",
    "\n",
    "        latent_dim_pi = self.mlp_extractor.latent_dim_pi\n",
    "\n",
    "        if isinstance(self.action_dist, DiagGaussianDistribution):\n",
    "            self.action_net, self.log_std = self.action_dist.proba_distribution_net(\n",
    "                latent_dim=latent_dim_pi, log_std_init=self.log_std_init\n",
    "            )\n",
    "        elif isinstance(self.action_dist, StateDependentNoiseDistribution):\n",
    "            self.action_net, self.log_std = self.action_dist.proba_distribution_net(\n",
    "                latent_dim=latent_dim_pi, latent_sde_dim=latent_dim_pi, log_std_init=self.log_std_init\n",
    "            )\n",
    "        elif isinstance(self.action_dist, (CategoricalDistribution, MultiCategoricalDistribution, BernoulliDistribution)):\n",
    "            self.action_net = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported distribution '{self.action_dist}'.\")\n",
    "\n",
    "        self.value_net = nn.Linear(self.mlp_extractor.latent_dim_vf, self.env_num_rewards)\n",
    "        # Init weights: use orthogonal initialization\n",
    "        # with small initial weight for the output\n",
    "        if self.ortho_init:\n",
    "            # TODO: check for features_extractor\n",
    "            # Values from stable-baselines.\n",
    "            # features_extractor/mlp values are\n",
    "            # originally from openai/baselines (default gains/init_scales).\n",
    "            module_gains = {\n",
    "                self.features_extractor: np.sqrt(2),\n",
    "                self.mlp_extractor: np.sqrt(2),\n",
    "                self.action_net: 0.01,\n",
    "                self.value_net: 1,\n",
    "            }\n",
    "            if not self.share_features_extractor:\n",
    "                # Note(antonin): this is to keep SB3 results\n",
    "                # consistent, see GH#1148\n",
    "                del module_gains[self.features_extractor]\n",
    "                module_gains[self.pi_features_extractor] = np.sqrt(2)\n",
    "                module_gains[self.vf_features_extractor] = np.sqrt(2)\n",
    "\n",
    "            for module, gain in module_gains.items():\n",
    "                module.apply(partial(self.init_weights, gain=gain))\n",
    "\n",
    "        # Setup optimizer with initial learning rate\n",
    "        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stable_baselines3' from 'C:\\\\Users\\\\callu\\\\anaconda3\\\\lib\\\\site-packages\\\\stable_baselines3\\\\__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import stable_baselines3\n",
    "importlib.reload(stable_baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "In CustomMlpExtractor\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -3.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(9.7789e-09, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0058, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0121, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0214, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0365, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0325, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0418, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0718, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0545, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0716, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0815, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1044, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0895, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.0806, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1054, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1119, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1224, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1239, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1337, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1334, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1169, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1203, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1255, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1134, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1050, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1251, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1303, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1250, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1432, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1300, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1187, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1370, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1292, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1561, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1453, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1367, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1367, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1355, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1409, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1381, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1365, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1418, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1303, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1407, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1489, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1506, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1456, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1481, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1350, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1280, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1326, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1421, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1471, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1449, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1350, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1456, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1238, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1558, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1435, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1468, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1352, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1329, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1486, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1465, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1554, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1388, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1344, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1582, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1409, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1452, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1096, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1504, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1296, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1477, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1570, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1371, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1514, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1542, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1434, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1250, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1240, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1364, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1529, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1359, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1321, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1430, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1384, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1459, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1388, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1468, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1491, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1495, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1487, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1395, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1461, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1498, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1510, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1350, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1274, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1415, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1291, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1474, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1470, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1562, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1527, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1415, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1558, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1561, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1504, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1455, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1523, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1449, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1456, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1532, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1405, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1310, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1330, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1491, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1422, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1485, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1502, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1391, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1255, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1345, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1435, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1393, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1331, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1402, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1386, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1514, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1428, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1369, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1359, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1411, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1496, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1474, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1275, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1451, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1523, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1594, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1394, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1411, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1469, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1328, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1254, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1440, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1398, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1452, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1420, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1521, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1508, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1395, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1417, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1430, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1463, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1554, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1273, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1509, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1497, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1255, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1428, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1510, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1413, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1371, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1369, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1515, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1400, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1519, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1530, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1574, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1331, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1338, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1387, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1215, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1423, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1498, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1394, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1425, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1463, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1314, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1537, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1497, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1431, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1458, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1469, grad_fn=<NegBackward0>)\n",
      "advantages.shape: torch.Size([64])\n",
      "ratio.shape: torch.Size([64])\n",
      "policy_loss_1.shape: torch.Size([64])\n",
      "doing original\n",
      "policy_loss.shape: torch.Size([])\n",
      "policy_loss: tensor(-0.1469, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d1d7931dfc10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0miters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel_sing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTIMESTEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"PPO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     ) -> SelfPPO:\n\u001b[1;32m--> 323\u001b[1;33m         return super().learn(\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[1;31m# Optimization step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m                 \u001b[1;31m# Clip grad norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env_sing = CustomEnvSing()\n",
    "env_sing.reset()\n",
    "\n",
    "model_sing = PPO(CustomActorCriticPolicy, env_sing, verbose=1)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "    model_sing.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "In CustomMlpExtractor\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -1.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "advantages.shape: torch.Size([64, 2])\n",
      "ratio.shape: torch.Size([64, 2])\n",
      "policy_loss_1.shape: torch.Size([64, 2])\n",
      "doing modified\n",
      "policy_loss.shape: torch.Size([2])\n",
      "policy_loss: tensor([ 0.1749, -0.1749], grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5e59c7a0d9fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0miters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTIMESTEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"PPO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     ) -> SelfPPO:\n\u001b[1;32m--> 323\u001b[1;33m         return super().learn(\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;31m# Logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                 \u001b[0mpg_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m                 \u001b[0mclip_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 \u001b[0mclip_fractions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = CustomEnv()\n",
    "env.reset()\n",
    "\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=1)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = th.tensor([[1, 2], [3, 4], [5, 6]], dtype=th.float32)\n",
    "rati = th.tensor([1, 1, 1])\n",
    "te * rati.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te * rati.repeat(2, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-3, -3, -3]), tensor([-3, -3, -3])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[th.tensor([1, 2, 3]) - th.tensor([4, 5, 6])] * 2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.exp(th.tensor([1, 2, 3]) - th.tensor([4, 5, 6])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.policy.mlp_extractor.policy_net)\n",
    "print(model.policy.mlp_extractor.value_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassB():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def printer(self):\n",
    "        print(\"Yo\")\n",
    "\n",
    "class ClassA():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def run_class_b(self):\n",
    "        dummy_class_b = ClassB()\n",
    "        dummy_class_b.printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_class_a = ClassA()\n",
    "dummy_class_a.run_class_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"Before function call\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(\"After function call\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def my_function():\n",
    "    print(\"Inside my_function\")\n",
    "\n",
    "# Manually apply the decorator when calling the function\n",
    "decorated_function = decorator(my_function)\n",
    "decorated_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_printer(self):\n",
    "    print(\"Yeah\")\n",
    "\n",
    "def modified_b_printer(printer):\n",
    "    def wrapper(self):\n",
    "        #self.printer = new_printer\n",
    "        #self.printer(self)\n",
    "        new_printer(self)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_class_b = ClassB()\n",
    "decorated_method = modified_b_printer(dum_class_b.printer)\n",
    "decorated_method(dum_class_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_class_b = ClassB()\n",
    "\n",
    "#modified_b_printer(dum_class_b.printer())\n",
    "class NewClassB(ClassB):\n",
    "    @modified_b_printer\n",
    "    def printer(self):\n",
    "        super.printer()\n",
    "\n",
    "dum_class_b = NewClassB()\n",
    "dum_class_b.printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_a_method(printer):\n",
    "    def wrapper(self):\n",
    "        @modified_b_printer\n",
    "        def printer(self):\n",
    "            self.printer = new_printer\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class NewClassA(ClassA):\n",
    "    @modified_a_method\n",
    "    def run_class_b(self):\n",
    "        super.run_class_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_class_a = NewClassA()\n",
    "dummy_class_a.run_class_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_method(original_method):\n",
    "    def wrapper(self, x, y):\n",
    "        # Call the original method with the modified arguments\n",
    "        result = original_method(self, x, y)\n",
    "\n",
    "        # Add code to modify the result or perform additional operations if needed\n",
    "        modified_result = result + 10\n",
    "\n",
    "        # Return the modified result\n",
    "        return modified_result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "class MyClass:\n",
    "    @modify_method\n",
    "    def my_method(self, x, y):\n",
    "        # Original method logic\n",
    "        return x + y\n",
    "\n",
    "# Create an instance of MyClass\n",
    "my_instance = MyClass()\n",
    "\n",
    "# Call the modified method\n",
    "my_instance.my_method(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassC1:\n",
    "    def printer(self):\n",
    "        print(\"Yo\")\n",
    "\n",
    "class ClassB1:\n",
    "    def __init__(self):\n",
    "        self.class_c1_instance = ClassC1()\n",
    "    \n",
    "    def call_class_c1(self):\n",
    "        self.class_c1_instance.printer()\n",
    "        \n",
    "\n",
    "class ClassA1:\n",
    "    def create_and_call_class_b1(self):\n",
    "        class_b1_instance = ClassB1()\n",
    "        class_b1_instance.call_class_c1()\n",
    "\n",
    "class_a1_instance = ClassA1()\n",
    "class_a1_instance.create_and_call_class_b1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since ClassA1.create_and_call_class_b1 creates and calls in one function, cant use a wrapper so need to use inheritance.\n",
    "class NewClassB1(ClassB1):\n",
    "    def __init__(self):\n",
    "        super.__init__(self)\n",
    "        print(\"in new class\")\n",
    "    \n",
    "    def call_class_c1(self):\n",
    "        self.class_c1_instance.printer()\n",
    "\n",
    "\n",
    "# Define a decorator and wrapper that will change the method create_and_call_class_b1 so that it thinks ClassB1 is different?\n",
    "def modify_create_and_call_class_b1(create_and_call_class_b1):\n",
    "    def wrapper(self):\n",
    "        ClassB1 = NewClassB1\n",
    "        self.create_and_call_class_b1()\n",
    "    return wrapper\n",
    "\n",
    "class_a1_instance = ClassA1()\n",
    "decorated_method = modify_create_and_call_class_b1(class_a1_instance.create_and_call_class_b1)\n",
    "decorated_method(class_a1_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewClassA1(ClassA1):\n",
    "    @modify_create_and_call_class_b1\n",
    "    def create_and_call_class_b1(self):\n",
    "        class_b1_instance = ClassB1()\n",
    "        class_b1_instance.call_class_c1()\n",
    "\n",
    "te = NewClassA1()\n",
    "te.create_and_call_class_b1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassC2:\n",
    "    def printer(self):\n",
    "        print(\"Yo\")\n",
    "\n",
    "class ClassB2:\n",
    "    def __init__(self):\n",
    "        self.class_c2_instance = ClassC2()\n",
    "    \n",
    "    def call_class_c2(self):\n",
    "        self.class_c2_instance.printer()\n",
    "        \n",
    "\n",
    "class ClassA2:\n",
    "    def __init__(self):\n",
    "        self.class_b2_instance = ClassB2()\n",
    "    \n",
    "    def call_class_b2(self):\n",
    "        self.class_b2_instance.call_class_c2()\n",
    "\n",
    "class_a2_instance = ClassA2()\n",
    "class_a2_instance.call_class_b2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_printer(self):\n",
    "    print(\"Yeah\")\n",
    "\n",
    "def modify_call_class_c2(call_class_c2):\n",
    "    def wrapper(self):\n",
    "        new_printer(self)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "def modify_call_class_b2(call_class_b2):\n",
    "    def wrapper(self):\n",
    "        decorated_method = modify_call_class_c2(self.call_class_b2)\n",
    "        decorated_method(self.class_b2_instance)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class_a2_instance = ClassA2()\n",
    "decorated_method = modify_call_class_b2(class_a2_instance.call_class_b2)\n",
    "decorated_method(class_a2_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassB():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def printer(self):\n",
    "        print(\"Yo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_printer(self):\n",
    "    print(\"Yeah\")\n",
    "\n",
    "def modified_b_printer(printer):\n",
    "    def wrapper(self):\n",
    "        #self.printer = new_printer\n",
    "        #self.printer(self)\n",
    "        new_printer(self)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_class_b = ClassB()\n",
    "decorated_method = modified_b_printer(dum_class_b.printer)\n",
    "decorated_method(dum_class_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassBTEMP():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def printer(self):\n",
    "        print(\"Yo\")\n",
    "\n",
    "def modified_b_printerTEMP(printer):\n",
    "    def wrapper(self):\n",
    "        return wrapper\n",
    "\n",
    "dum_class_b = ClassB()\n",
    "decorated_method = modified_b_printer(dum_class_b.printer)\n",
    "decorated_method(dum_class_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
